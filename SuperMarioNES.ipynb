{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SuperMarioNES.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsLJZ+A5ctxrMA78WndfWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamnassim/AAI510/blob/main-menu/SuperMarioNES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Deep Q-Network with Super Mario NES"
      ],
      "metadata": {
        "id": "duMJ-xl9_IkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import Super Mario Bros Library "
      ],
      "metadata": {
        "id": "przGUfTS_IIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym_super_mario_bros\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "qoQNt07B8JT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call env variable for environment"
      ],
      "metadata": {
        "id": "Cmmkvqv2_5vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "print(env.observation_space.shape)  # Dimensions of a frame\n",
        "print(env.action_space.n)  # Number of actions our agent can take"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlydB86g-WKH",
        "outputId": "c9feba1d-cac4-49eb-8959-b2ec29460931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3)\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent take 256 different actions"
      ],
      "metadata": {
        "id": "EXX0e2sz_xc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Help Agent Learn Faster"
      ],
      "metadata": {
        "id": "gcM0bsyN-7xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env):\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    return JoypadSpace(env, RIGHT_ONLY)"
      ],
      "metadata": {
        "id": "cP0tlK7y_Dtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an agent for Super Mario Bros (NES)"
      ],
      "metadata": {
        "id": "3720nBVGAwdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build DQN architecture"
      ],
      "metadata": {
        "id": "egcKxv9vBnqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNSolver(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQNSolver, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "3BYavNcl_FkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Main Function/Execute Function"
      ],
      "metadata": {
        "id": "sLnqHmZGBsoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "    env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
        "    observation_space = env.observation_space.shape\n",
        "    action_space = env.action_space.n\n",
        "    agent = DQNAgent(state_space=observation_space,\n",
        "                     action_space=action_space,\n",
        "                     max_memory_size=30000,\n",
        "                     batch_size=32,\n",
        "                     gamma=0.90,\n",
        "                     lr=0.00025,\n",
        "                     exploration_max=0.02,\n",
        "                     exploration_min=0.02,\n",
        "                     exploration_decay=0.99)\n",
        "    \n",
        "    num_episodes = 10000\n",
        "    env.reset()\n",
        "    total_rewards = []\n",
        "    \n",
        "    for ep_num in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        state = torch.Tensor([state])\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            \n",
        "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
        "            total_reward += reward\n",
        "            state_next = torch.Tensor([state_next])\n",
        "            reward = torch.tensor([reward]).unsqueeze(0)\n",
        "            \n",
        "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
        "            agent.remember(state, action, reward, state_next, terminal)\n",
        "            agent.experience_replay()\n",
        "            \n",
        "            state = state_next\n",
        "            if terminal:\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
        "        num_episodes += 1 "
      ],
      "metadata": {
        "id": "Ygcjal4wS-Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the most important methods of our agent: remember, recall, and experience_replay."
      ],
      "metadata": {
        "id": "caLy67qFTmQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    ...\n",
        "    def remember(self, state, action, reward, state2, done):\n",
        "        self.STATE_MEM[self.ending_position] = state.float()\n",
        "        self.ACTION_MEM[self.ending_position] = action.float()\n",
        "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
        "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
        "        self.DONE_MEM[self.ending_position] = done.float()\n",
        "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
        "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
        "        \n",
        "    def recall(self):\n",
        "        # Randomly sample 'batch size' experiences\n",
        "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
        "        \n",
        "        STATE = self.STATE_MEM[idx].to(self.device)\n",
        "        ACTION = self.ACTION_MEM[idx].to(self.device)\n",
        "        REWARD = self.REWARD_MEM[idx].to(self.device)\n",
        "        STATE2 = self.STATE2_MEM[idx].to(self.device)\n",
        "        DONE = self.DONE_MEM[idx].to(self.device)\n",
        "        \n",
        "        return STATE, ACTION, REWARD, STATE2, DONE\n",
        "        \n",
        "    def experience_replay(self):\n",
        "        \n",
        "        if self.step % self.copy == 0:\n",
        "            self.copy_model()\n",
        "\n",
        "        if self.memory_sample_size > self.num_in_queue:\n",
        "            return\n",
        "\n",
        "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        # Double Q-Learning target is Q*(S, A) <- r + Î³ max_a Q_target(S', a)\n",
        "        target = REWARD + torch.mul((self.gamma * \n",
        "        self.target_net(STATE2).max(1).values.unsqueeze(1)), \n",
        "        1 - DONE)\n",
        "\n",
        "        current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
        "        \n",
        "        loss = self.l1(current, target)\n",
        "        loss.backward() # Compute gradients\n",
        "        self.optimizer.step() # Backpropagate error\n"
      ],
      "metadata": {
        "id": "cOxmzqswTprF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}